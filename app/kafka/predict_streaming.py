import os
import logging
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType, DoubleType

from configs.config_loader import load_config
from configs.enum_headers import RawColumns
from preprocessing.pipeline import run_prediction_pipeline, config

# === åˆå§‹åŒ– log ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH = os.path.join(BASE_DIR, '../configs/paths.yaml')
config = load_config(CONFIG_PATH, project_base=BASE_DIR, use_hdfs=True)
ts = datetime.now().strftime('%Y%m%d_%H%M%S')

logging.basicConfig(
    filename=f"kafka_predict_{ts}.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

console = logging.StreamHandler()
console.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console)

# === Kafka çµæ§‹ schema ===
schema = StructType() \
    .add(RawColumns.Student_ID.value, StringType()) \
    .add(RawColumns.Gender.value, StringType()) \
    .add(RawColumns.Extracurricular_Activities.value, StringType()) \
    .add(RawColumns.Internet_Access_at_Home.value, StringType()) \
    .add(RawColumns.Family_Income_Level.value, StringType()) \
    .add(RawColumns.Parent_Education_Level.value, StringType()) \
    .add(RawColumns.Department.value, StringType()) \
    .add(RawColumns.Grade.value, StringType()) \
    .add(RawColumns.Study_Hours_per_Week.value, DoubleType()) \
    .add(RawColumns.Final_Score.value, DoubleType())

def safe_run_prediction(df, batch_id):
    logger.info(f"ğŸ“¦ æ¥æ”¶åˆ°æ–°çš„ batch_id={batch_id}ï¼Œè³‡æ–™ç­†æ•¸: {df.count()}")
    try:
        run_prediction_pipeline(df, batch_id)
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œé æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤ (batch_id={batch_id})", exc_info=True)

# === å»ºç«‹ Spark Streaming ä»»å‹™ ===
try:
    spark = SparkSession.builder.appName("KafkaPredictStream").getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    logger.info("âœ… Spark Session å»ºç«‹æˆåŠŸ")

    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:9092") \
        .option("subscribe", "predict_topic") \
        .option("startingOffsets", "earliest") \
        .load()

    raw_df = kafka_df.selectExpr("CAST(value AS STRING) as json_str")
    logger.info("ğŸ” Kafka è¨‚é–±æˆåŠŸï¼Œæº–å‚™è§£æ JSON schema")

    # debug log of raw json
    debug_query = raw_df.writeStream \
        .format("console") \
        .option("truncate", False) \
        .start()

    parsed_df = raw_df.select(from_json(col("json_str"), schema).alias("data")).select("data.*")

    logger.info("ğŸš€ Kafka streaming é–‹å§‹åŸ·è¡Œé æ¸¬ä»»å‹™...")

    main_query = parsed_df.writeStream \
        .foreachBatch(safe_run_prediction) \
        .start()

    logger.info(f"ğŸ¯ Streaming å•Ÿå‹•æˆåŠŸ: isActive={main_query.isActive}")
    main_query.awaitTermination()

except Exception as e:
    logger.error("ğŸ”¥ Kafka Spark Streaming å•Ÿå‹•å¤±æ•—", exc_info=True)
