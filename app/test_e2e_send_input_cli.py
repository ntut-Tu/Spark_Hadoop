import json
import time
from kafka import KafkaProducer, KafkaConsumer
from pyspark.sql import SparkSession
from configs.enum_headers import RawColumns

def test_end_to_end_prediction():
    # 1ï¸âƒ£ æº–å‚™æ¸¬è©¦è³‡æ–™
    test_record = {
        RawColumns.Student_ID.value: "E2E_123",
        RawColumns.Gender.value: "Male",
        RawColumns.Extracurricular_Activities.value: "Yes",
        RawColumns.Internet_Access_at_Home.value: "Yes",
        RawColumns.Family_Income_Level.value: "Medium",
        RawColumns.Parent_Education_Level.value: "Bachelor's",
        RawColumns.Department.value: "CS",
        RawColumns.Grade.value: "A",
        RawColumns.Study_Hours_per_Week.value: 15.0,
        RawColumns.Final_Score.value: 90.0
    }

    # 2ï¸âƒ£ Kafka Producer ç™¼é€
    producer = KafkaProducer(
        bootstrap_servers='kafka:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    producer.send("predict_topic", test_record)
    producer.flush()
    print("âœ… æ¸¬è©¦è³‡æ–™å·²é€å‡º")

    # 3ï¸âƒ£ Kafka Consumer ç­‰å¾…çµæœ
    consumer = KafkaConsumer(
        'predict_result_topic',
        bootstrap_servers='kafka:9092',
        group_id='e2e-test-group',
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    print("ğŸ§ ç­‰å¾…çµæœä¸­...")
    start_time = time.time()
    timeout = 20

    while time.time() - start_time < timeout:
        for message in consumer:
            result = message.value
            if result.get(RawColumns.Student_ID.value) == "E2E_123":
                print("ğŸ¯ æ¥æ”¶åˆ°é æ¸¬çµæœï¼š", result)
                assert "score_cluster" in result
                assert "background_cluster" in result
                return

    raise TimeoutError("âŒ è¶…é 20 ç§’æœªæ¥æ”¶åˆ°é æ¸¬çµæœ")
