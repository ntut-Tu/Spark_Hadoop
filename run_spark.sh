#!/bin/bash

# è·¯å¾‘åŸºåº•
BASE_DIR=$(dirname "$(readlink -f "$0")")
RAW_LOCAL_PATH="$BASE_DIR/data/raw/Students_Grading_Dataset.csv"
RAW_HDFS_PATH="/data/raw/Students_Grading_Dataset.csv"

# æª¢æŸ¥æœ¬åœ°æª”æ¡ˆæ˜¯å¦å­˜åœ¨
if [ ! -f "$RAW_LOCAL_PATH" ]; then
  echo "âŒ æ‰¾ä¸åˆ°æœ¬åœ°è³‡æ–™: $RAW_LOCAL_PATH"
  exit 1
fi

# æª¢æŸ¥ HDFS æ˜¯å¦å·²æœ‰æª”æ¡ˆï¼Œè‹¥ç„¡å‰‡è‡ªå‹•ä¸Šå‚³
hdfs dfs -test -e "$RAW_HDFS_PATH"
if [ $? -ne 0 ]; then
  echo "ğŸ“¤ ä¸Šå‚³è³‡æ–™åˆ° HDFS: $RAW_HDFS_PATH"
  hdfs dfs -mkdir -p "$(dirname "$RAW_HDFS_PATH")"
  hdfs dfs -put "$RAW_LOCAL_PATH" "$RAW_HDFS_PATH"
else
  echo "âœ… HDFS æª”æ¡ˆå·²å­˜åœ¨ï¼Œç•¥éä¸Šå‚³"
fi

# åŸ·è¡Œ Spark ä»»å‹™
spark-submit \
  --master spark://192.168.100.10:7077 \
  --conf spark.driver.host=192.168.100.10 \
  main.py

echo "ğŸ“¥ å°‡ HDFS çµæœä¸‹è¼‰åˆ°æœ¬åœ°..."

# å»ºç«‹æœ¬åœ°è³‡æ–™å¤¾
mkdir -p output/interim output/processed output/crosstab

# å¾ HDFS è¤‡è£½ parquet çµæœ
hdfs dfs -get / output/

# è¤‡è£½äº¤å‰è¡¨çµæœï¼ˆcsv æ˜¯ç›®éŒ„å½¢å¼ï¼‰
hdfs dfs -getmerge /data/processed/cluster_crosstab.csv output/crosstab/cluster_crosstab.csv

echo "âœ… çµæœå·²ä¿å­˜è‡³æœ¬åœ° output/ è³‡æ–™å¤¾"
